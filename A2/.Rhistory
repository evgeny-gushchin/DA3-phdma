data$last_scraped <- as.Date(data$last_scraped)
data$first_review <- as.Date(data$first_review)
data$last_review <- as.Date(data$last_review)
data$days_since_first_review <- data$last_scraped - data$first_review
table(data$days_since_first_review)
g29 <- ggplot(data, aes(x=days_since_first_review, y=price_numeric)) +
geom_point(alpha=0.05)+ theme_minimal() +
labs(x = "Days since first review",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))
g29
data$days_since_last_review <- data$last_scraped - data$last_review
table(data$days_since_last_review)
#cov(as.numeric(data$days_since_first_review), data$days_since_last_review)
g30 <- ggplot(data, aes(x=days_since_last_review, y=price_numeric)) +
geom_point(alpha=0.05)+ theme_minimal() +
labs(x = "Days since last review",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))
g30
table(data$review_scores_rating)
g31 <- ggplot(data, aes(x=review_scores_rating, y=price_numeric)) +
geom_point(alpha=0.05)+ theme_minimal() +
labs(x = "Review rating",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))
g31
g31a <- ggplot(data, aes(x = factor(round(data$review_scores_rating,1)), y = price_numeric,
)) +
geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
labs(x = "Review rating",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))+
theme_minimal()
g31a
table(data$review_scores_value)
g32 <- ggplot(data, aes(x=review_scores_value, y=price_numeric)) +
geom_point(alpha=0.05)+ theme_minimal() +
labs(x = "Review value",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))
g32
g32a <- ggplot(data, aes(x = factor(round(data$review_scores_value,1)), y = price_numeric,
)) +
geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
labs(x = "Review value",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))+
theme_minimal()
g32a
# Specify the variables for which you want to calculate correlations
selected_variables <- c(
"review_scores_rating",
"review_scores_accuracy",
"review_scores_cleanliness",
"review_scores_checkin",
"review_scores_communication",
"review_scores_location",
"review_scores_value"
)
# Extract the selected variables from the data frame
selected_data <- data[, selected_variables]
# Chat GPT help
selected_data <- selected_data[complete.cases(selected_data), ]
# Calculate pairwise correlations
correlation_matrix4 <- cor(selected_data)
# Print the correlation matrix
print(correlation_matrix4) # somewhat correlated, but can use all
table(data$instant_bookable)
g33 <- ggplot(data, aes(x = factor(instant_bookable), y = price_numeric,
)) +
geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
labs(x = "Bookable now?",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))+
theme_minimal()
g33
table(data$calculated_host_listings_count)
data$calculated_host_listings_count[data$calculated_host_listings_count > 9] <- 10
#may be relevant
g34 <- ggplot(data, aes(x = factor(calculated_host_listings_count), y = price_numeric,
)) +
geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
labs(x = "Host's listings count",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))+
theme_minimal()
g34
cor(data$calculated_host_listings_count, data$host_listings_count) # very correlated probably no need
table(data$calculated_host_listings_count_entire_homes)
#may be relevant
table(data$calculated_host_listings_count_private_rooms)
#doesn't seem very relevant
table(data$calculated_host_listings_count_shared_rooms)
summary(data$reviews_per_month) # it seems that the NA value here is simply 0
data$reviews_per_month_flag <- 0
data$reviews_per_month_flag[is.na(data$reviews_per_month)] <- 1 #creating a flag variable
data$reviews_per_month[is.na(data$reviews_per_month)] <- 0
summary(data$reviews_per_month)
g35 <- ggplot(data, aes(x = factor(reviews_per_month), y = price_numeric,
)) +
geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
labs(x = "Reviews per month",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))+
theme_minimal()
g35
g35a <- ggplot(data, aes(x=reviews_per_month, y=price_numeric)) +
geom_point(alpha=0.05)+ theme_minimal() +
labs(x = "Reviews per month",y = "Apartment price")+
scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 200), breaks = seq(0,200, 20))
g35a
# List of column names you want to convert to factors
columns_to_factor <- c("host_response_time", "host_is_superhost", "host_listings_count", "neighbourhood_cleansed", "property_type", "wifi")
# Use lapply to apply as.factor to the specified columns
data[columns_to_factor] <- lapply(data[columns_to_factor], as.factor)
data %>% glimpse()
data <- data %>% mutate(accommodates_sq = accommodates^2, bathroom_sq = bathroom^2, beds_sq = beds^2, bedrooms_sq = bedrooms^2, amenity_length_sq = amenity_length^2) #days_since_first_review_sq = days_since_first_review^2, days_since_last_review_sq = days_since_last_review^2)
continuous_vars <- c("accommodates","accommodates_sq","bathroom","bathroom_sq","beds","beds_sq","bedrooms","bedrooms_sq", "amenity_length", "amenity_length_sq") #"days_since_first_review", "days_since_first_review_sq","days_since_last_review","days_since_last_review_sq")
# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(data))
# Set the random number generator: It will make results reproducable
set.seed(20180123)
# create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$holdout <- 0
data$holdout[holdout_ids] <- 1
#Hold-out set Set
data_holdout <- data %>% filter(holdout == 1)
#Working data set
data_work <- data %>% filter(holdout == 0)
#### First model: OLS + Lasso #####
# take model 8 (and find observations where there is no missing data)may
vars_model_7 <- c("price_numeric", continuous_vars)
vars_model_8 <- c("price_numeric", continuous_vars, columns_to_factor)
# Set lasso tuning parameters
n_folds=5
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# We use model 7 without the interactions so that it is easy to compare later to post lasso ols
formula <- formula(paste0("price_numeric ~ ", paste(setdiff(vars_model_8, "price_numeric"), collapse = " + ")))
set.seed(1234)
lasso_model <- caret::train(formula,
data = data_work,
method = "glmnet",
preProcess = c("center", "scale"),
trControl = train_control,
tuneGrid = tune_grid,
na.action=na.exclude)
print(lasso_model$bestTune$lambda)
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column(var = "variable") %>%
rename(coefficient = `s1`)  # the column has a name "1", to be renamed
print(lasso_coeffs)
lasso_coeffs_nz<-lasso_coeffs %>%
filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
# We use model 7 without the interactions so that it is easy to compare later to post lasso ols
formula <- formula(paste0("price_numeric ~ ", paste(setdiff(vars_model_7, "price_numeric"), collapse = " + ")))
set.seed(1234)
lasso_model <- caret::train(formula,
data = data_work,
method = "glmnet",
preProcess = c("center", "scale"),
trControl = train_control,
tuneGrid = tune_grid,
na.action=na.exclude)
print(lasso_model$bestTune$lambda)
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column(var = "variable") %>%
rename(coefficient = `s1`)  # the column has a name "1", to be renamed
print(lasso_coeffs)
lasso_coeffs_nz<-lasso_coeffs %>%
filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
#### First model: OLS + Lasso #####
# take model 8 (and find observations where there is no missing data)may
#vars_model_7 <- c("price_numeric", continuous_vars)
vars_model_1 <- c("price_numeric", continuous_vars, columns_to_factor)
# Set lasso tuning parameters
n_folds=5
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# We use model 7 without the interactions so that it is easy to compare later to post lasso ols
formula <- formula(paste0("price_numeric ~ ", paste(setdiff(vars_model_1, "price_numeric"), collapse = " + ")))
set.seed(1234)
lasso_model <- caret::train(formula,
data = data_work,
method = "glmnet",
preProcess = c("center", "scale"),
trControl = train_control,
tuneGrid = tune_grid,
na.action=na.exclude)
print(lasso_model$bestTune$lambda)
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column(var = "variable") %>%
rename(coefficient = `s1`)  # the column has a name "1", to be renamed
print(lasso_coeffs)
lasso_coeffs_nz<-lasso_coeffs %>%
filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
filter(lambda == lasso_model$bestTune$lambda) %>%
dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])
model3_level <- model_results_cv[["modellev3"]][["model_work_data"]]
model7_level_holdout_rmse <- mse_lev(predict(model7_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)
library(skimr)
#install.packages("installr")
#installr::updateR()
#install.packages("remotes")
install.packages("skimr")
library(skimr)
install.packages("skimr")
library(directlabels)
#install.packages("installr")
#installr::updateR()
#install.packages("remotes")
#install.packages("skimr")
install.packages("directlabels")
library(directlabels)
model7_level_holdout_rmse <- mse_lev(predict(model7_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)
library(cowplot)
#install.packages("installr")
#installr::updateR()
#install.packages("remotes")
#install.packages("skimr")
#install.packages("directlabels")
install.packages("cowplot")
library(cowplot)
model7_level_holdout_rmse <- mse_lev(predict(model7_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)
data_holdout[,"price_numeric"]
model_results_cv <- list()
model1_level <- model_results_cv[["modellev1"]][["model_work_data"]]
model1_level
model_results_cv
# look at holdout RMSE
# look at holdout RMSE
model1_level_holdout_rmse <- mse_lev(predict(model1_level, newdata = data_holdout), data_holdout[,"price_numeric"] %>% pull)**(1/2)
predict(model1_level, newdata = data_holdout)
lasso_cv_rmse
# Use the LASSO model to predict price_numeric for the holdout set
predictions_holdout <- predict(lasso_model, newdata = data_holdout)
# Print or check the predictions
print(predictions_holdout)
dim(predictions_holdout)
length(predictions_holdout)
legth(data_holdout)
legth(data_holdout$price_numeric)
length(data_holdout$price_numeric)
sum((predictions_holdout-data_holdout$price_numeric)^2)
sqrt(sum((predictions_holdout-data_holdout$price_numeric)^2))
# Preprocess the holdout data (center and scale)
data_holdout_processed <- predict(lasso_model$preProcess, newdata = data_holdout)
# Use the LASSO model to predict price_numeric for the holdout set
predictions_holdout <- predict(lasso_model, newdata = data_holdout_processed)
# Preprocess the holdout data (center and scale)
data_holdout_processed <- predict(lasso_model$preProcess, newdata = data_holdout)
lasso_model$preProcess
# Extract predictors from data_holdout
data_holdout_predictors <- data_holdout[, setdiff(colnames(data_holdout), "price_numeric")]
# Preprocess the holdout data (center and scale)
data_holdout_processed <- predict(lasso_model$preProcess, newdata = data_holdout_predictors)
# Preprocess the holdout data (center and scale)
data_holdout_processed <- predict(lasso_model$preProcess, newdata = data_holdout_predictors)
# Extract predictors from data_holdout
data_holdout_predictors <- data_holdout[, setdiff(colnames(data_holdout), "price_numeric")]
data_holdout_predictors
glimpse(data_holdout_predictors)
print(lasso_coeffs)
# Extract predictors from data_holdout
data_holdout_predictors <- data_holdout[, c("price_numeric", continuous_vars, columns_to_factor)]
glimpse(data_holdout_predictors)
setdiff(vars_model_1, "price_numeric")
# Extract predictors from data_holdout
data_holdout_predictors <- data_holdout[, c(continuous_vars, columns_to_factor)]
# Preprocess the holdout data (center and scale)
data_holdout_processed <- predict(lasso_model$preProcess, newdata = data_holdout_predictors)
lasso_model$preProcess
data %>% glimpse() # now we are down to 10,409 observations
table(data$source) # not sure how this variable can be useful
table(data$host_listings_count)
# Extract predictors from data_holdout
data_holdout_predictors <- data_holdout[, c(continuous_vars, columns_to_factor)]
# Preprocess the holdout data (center and scale)
data_holdout_processed <- predict(lasso_model$preProcess, newdata = data_holdout_predictors)
holdout_colnames <- colnames(data_holdout)
holdout_colnames
holdout_colnames <- colnames(data_holdout[, c(continuous_vars, columns_to_factor)])
holdout_colnames
options(digits = 3)
#### First model: OLS + Lasso #####
# take model 8 (and find observations where there is no missing data)may
#vars_model_7 <- c("price_numeric", continuous_vars)
vars_model_1 <- c("price_numeric", continuous_vars, columns_to_factor)
# Set lasso tuning parameters
n_folds=5
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# We use model 7 without the interactions so that it is easy to compare later to post lasso ols
formula <- formula(paste0("price_numeric ~ ", paste(setdiff(vars_model_1, "price_numeric"), collapse = " + ")))
set.seed(1234)
lasso_model <- caret::train(formula,
data = data_work,
method = "glmnet",
preProcess = c("center", "scale"),
trControl = train_control,
tuneGrid = tune_grid,
na.action=na.exclude)
print(lasso_model$bestTune$lambda)
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column(var = "variable") %>%
rename(coefficient = `s1`)  # the column has a name "1", to be renamed
print(lasso_coeffs)
lasso_coeffs_nz<-lasso_coeffs %>%
filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
filter(lambda == lasso_model$bestTune$lambda) %>%
dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])
print(lasso_model$bestTune$lambda)
predict(lasso_model, s=lasso_model$bestTune$lambda, newdata = data_holdout)
# Print or check the predictions
predictions_holdout <- predict(lasso_model, s=lasso_model$bestTune$lambda, newdata = data_holdout)
# Print or check the predictions
length(predictions_holdout)
length(data_holdout$price_numeric)
sqrt(sum((predictions_holdout-data_holdout$price_numeric)^2))
predict(lasso_model, s=lasso_model$bestTune$lambda, newdata = data_holdout)
lasso_model
lasso_model$bestTune
# Print or check the predictions for the holdout
predictions_holdout <- predict(lasso_model, data_holdout)
# Print or check the predictions
length(predictions_holdout)
sqrt(sum((predictions_holdout-data_holdout$price_numeric)^2))
# look at holdout RMSE
model1_rmse <- sqrt(sum((predictions_holdout-data_holdout$price_numeric)^2))
model1_rmse
# do 5-fold CV
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = FALSE)
# set tuning
tune_grid <- expand.grid(
.mtry = c(5, 7, 9),
.splitrule = "variance",
.min.node.size = c(5, 10)
)
# simpler model for model A (1)
set.seed(1234)
system.time({
rf_model_1 <- train(
formula,
data = data_work,
method = "ranger",
trControl = train_control,
tuneGrid = tune_grid,
importance = "impurity"
)
})
library(ranger)
# simpler model for model A (1)
set.seed(1234)
system.time({
rf_model_1 <- train(
formula,
data = data_work,
method = "ranger",
trControl = train_control,
tuneGrid = tune_grid,
importance = "impurity"
)
})
rf_model_1
summary(rf_model_1)
data_holdout_w_prediction <- data_holdout %>%
mutate(predicted_price = predict(rf_model_1, newdata = data_holdout))
data_holdout_w_prediction
model2_rmse <- sqrt(sum((data_holdout_w_prediction$predicted_price-data_holdout_w_prediction$price_numeric)^2))
model2_rmse
model1_rmse
#=================== Task 3 =====================
##### Shapley values #####
devtools::install_github('ModelOriented/treeshap')
#=================== Task 3 =====================
##### Shapley values #####
install.packages("devtools")
install.packages("devtools")
devtools::install_github('ModelOriented/treeshap')
library(treeshap)
library(treeshap)
devtools::install_github('ModelOriented/treeshap')
install.packages("cli")
install.packages("cli")
devtools::install_github('ModelOriented/treeshap')
library(cli)
devtools::install_github('ModelOriented/treeshap')
library(treeshap)
remotes::install_cran("cli")
remotes::install_cran("cli", force = TRUE)
devtools::install_github('ModelOriented/treeshap')
#### Third model: Boosting #####
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), # complexity of the tree
n.trees = (4:10)*50, # number of iterations, i.e. trees
shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)
set.seed(1234)
#### Third model: Boosting #####
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), # complexity of the tree
n.trees = (4:10)*50, # number of iterations, i.e. trees
shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)
set.seed(1234)
system.time({
gbm_model <- train(formula,
data = data_work,
method = "gbm",
trControl = train_control,
verbose = FALSE,
tuneGrid = gbm_grid)
})
gbm_model <- train(formula,
data = data_work,
method = "gbm",
trControl = train_control,
verbose = FALSE,
tuneGrid = gbm_grid)
system.time({
gbm_model <- train(formula,
data = data_work,
method = "gbm",
trControl = train_control,
verbose = FALSE,
tuneGrid = gbm_grid)
})
system.time({
gbm_model <- train(formula,
data = data_work,
method = "gbm",
trControl = train_control,
verbose = FALSE,
tuneGrid = gbm_grid)
})
gbm_model
data_holdout_w_prediction_new <- data_holdout %>%
mutate(predicted_price = predict(gbm_model, newdata = data_holdout))
model3_rmse <- sqrt(sum((data_holdout_w_prediction_new$predicted_price-data_holdout_w_prediction_new$price_numeric)^2))
model3_rmse
model2_rmse
model1_rmse
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=data_holdout, fullRank=T, sep = NULL)
#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=data_holdout))
#From Chat GPT
shap_values <- treeshap(rf_model_1, X = data_holdout)
library(treeshap)
#From Chat GPT
shap_values <- treeshap(rf_model_1, X = data_holdout)
data_holdout
#From Chat GPT
shap_values <- treeshap(rf_model_1, X = data_holdout)
#From Chat GPT
# Assuming data_holdout is your holdout dataset
observation_to_visualize <- data_holdout[1, , drop = FALSE]  # Replace 1 with the index of the observation you want to visualize
# Compute Shapley values
shap_values <- treeshap(rf_model_1, X = observation_to_visualize)
data_holdout %>% glimpse()
data_holdout[:,26]
data_holdout[1,26]
data_holdout[3,26]
#From Chat GPT
# Assuming data_holdout is your holdout dataset
observation_to_visualize <- data_holdout[26, , drop = FALSE]  # Replace 1 with the index of the observation you want to visualize
# Compute Shapley values
shap_values <- treeshap(rf_model_1, X = observation_to_visualize)
help(treeshap)
# Compute Shapley values
shap_values <- treeshap(rf_model_1, x = observation_to_visualize)
# Compute Shapley values
shap_values <- treeshap(rf_model_1$finalModel, x = observation_to_visualize)
# Compute Shapley values
shap_values <- treeshap(rf_model_1, observation_to_visualize, type = "shapley")
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=data_holdout, fullRank=T, sep = NULL)
#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=data_holdout))
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=data_holdout$accommodates, fullRank=T, sep = NULL)
#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=data_holdout))
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=data_holdout$accommodates, fullRank=T, sep = NULL)
#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=data_holdout$accommodates))
# replace "." character to " " to match model object names
names(data_holdout_ohe) <- gsub(x = names(data_holdout_ohe),
pattern = "\\.",
replacement = " ")
dummy
